---
title: 'Math448: Chapter 4 HW'
author: "Zahraa Alshalal"
date: "2023-04-03"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ISLR2)
library(ISLR)
library(MASS)
library(class)
library(e1071)
```

## Conceptual Questions

### Exercise 3:

-   The Bayes classifier is not linear: quadratic $$\begin{aligned}
     p_k(x)=\frac{\pi_k\frac{1}{\sqrt{2\pi\sigma_k}}exp(-\frac{1}{2\sigma^2_k})(x-\mu_l)^2)}{\sum\pi_l\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{1}{2\sigma^2_l})(x-\mu_l)^2)}\\   
     \log{(p_k(x))}=\frac{\log{(\pi_k)}+\log{(\frac{1}{\sqrt{2\pi\sigma_k}}})+-\frac{1}{\sigma^2_k}(x-\mu_k)^2}{\log(\sum\pi_l\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{1}{2\sigma^2_l})(x-\mu_l)^2))}\\
     \log{(p_k(x))\log(\sum\pi_l\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{1}{2\sigma^2_l})(x-\mu_l)^2))=\log(\pi_k)+\log{(\frac{1}{\sqrt{2\pi\sigma_k}}})+-\frac{1}{\sigma^2_k}(x-\mu_k)^2}\\
     \delta(x)=\log(\pi_k)+\log{(\frac{1}{\sqrt{2\pi\sigma_k}}})+-\frac{1}{2\sigma^2_k}(x-\mu_k)^2\\
    \end{aligned}$$

-   $\delta(x)$ in the final term, the discriminant is not linear.

### Exercise 6:

-   

    a)  

$$
\begin{aligned}
P(X) &= \frac{\exp(\hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2)}{1 + \exp(\hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2)}\\
P(X) &= \frac{\exp(-0.5)}{1+\exp(-0.5)} = 0.38 
\end{aligned}
$$

-   

    b)  

$$
\begin{aligned}
\log(\frac{P(X)}{1 - P(X)}) &= \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2\\
\log(\frac{0.5}{1 - 0.5}) &= -6 + 0.05X1 + 3.5\\
X_1 &= 50 hours. 
\end{aligned}
$$

------------------------------------------------------------------------

## Applied Questions:

### Exercise 13:

#### a)

```{r}
view(Weekly)
summary(Weekly)
```

```{r}
# Scatterplot matrix.
pairs(Weekly)
```

```{r}
# Correlation matrix.
cor(Weekly[, -9])
```

-   The correlations between the "lag" variables and today's returns are close to zero. There is correlation is between "Year" and "Volume". No other patterns are discernible.\

```{r}
attach(Weekly)
plot(Volume)
```

-   The "Volume" plot shows that it is increasing over time.

#### b)

```{r}
fit.glm = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(fit.glm)
```

-   Lag 2 appears to have some statistical significance with a p-value is less than 0.05.

#### c)

```{r}
probs = predict(fit.glm, type = "response")
pred.glm = rep("Down", length(probs))
pred.glm[probs > 0.5] = "Up"
table(pred.glm, Direction)
```

```{r}
#the percentage of correct predictions on the training data
correct_predictions = (54+557)/(54+557+48+430)
correct_predictions
error_predictions = 1 - correct_predictions
error_predictions
market_up = (557)/(557+48)
market_up
market_down = 54/(54+430)
market_down
```

-   We may conclude that the percentage of correct predictions on the training data is 56.11%. The training error rate 43.89%. For the weeks when the market goes up, the model is right 92.07% of the time. For weeks when the market goes down, the model is right only 11.16%.

#### d)

```{r}
train = (Year < 2009)
Weekly.910 = Weekly[!train, ]
Direction.910 = Direction[!train]
fit.glm2 = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
summary(fit.glm2)
```

```{r}
probs2 = predict(fit.glm2, Weekly.910, type = "response")
pred.glm2 = rep("Down", length(probs2))
pred.glm2[probs2 > 0.5] <- "Up"
table(pred.glm2, Direction.910)
```

```{r}
#the percentage of correct predictions on the training data
correct_predictions1 = (9+56)/(104)
correct_predictions1
error_predictions1 = 1 - correct_predictions1
error_predictions1
market_up1 = (56)/(56+5)
market_up1
market_down1 = 9/(9+34)
market_down1
```

-   In this case, the percentage of correct predictions on the test data is 62.5%. the test error rate is 37.5%. For the weeks when the market goes up, the model is right 91.80% of the time. For the weeks when the market goes down, the model is right only 20.93% of the time.

#### e)

```{r}
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.fit
```

```{r}
lda.pred = predict(lda.fit, Weekly.910)
table(lda.pred$class, Direction.910)
```

```{r}
mean(lda.pred$class == Direction.910)
```

-   The percentage of correct predictions on the test data is 62.5%.

#### f)

```{r}
qda.fit = qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.fit 
pred.qda = predict(qda.fit, Weekly.910)$class
table(pred.qda, Direction.910)
mean(pred.qda == Direction.910)
```

-   The percentage of correct predictions on the test data is 58.7%.

#### g)

```{r}
train.X = as.matrix(Lag2[train])
test.X = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.910)
```

```{r}
mean(knn.pred == Direction.910)
```

-   The percentage of correct predictions on the test data is 50%.     
     
     
#### h)

```{r}
bayes.fit=naiveBayes(Direction~Lag2 ,data=Weekly ,subset=train)
bayes.fit
```

```{r}
bayes.pre = predict(bayes.fit, Weekly.910)
table(bayes.pre, Direction.910)
```

```{r}
correct_predictions1 = (61)/(104)
correct_predictions1
```

-   The percentage of correct predictions on the test data is 58.65%%.

#### i)

    The regression model percentage of correct predictions is 62.5% of the time which is the highest between all the models. The regression method appears to provide the best results.      

     

#### j)

```{r}
# Logistic regression with Lag2:Lag1
glm.fit = glm(Direction ~ Lag2:Lag1, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit, Weekly.910, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
Direction.0910 = Direction[!train]
table(glm.pred, Direction.910)
```

```{r}
mean(glm.pred == Direction.910)
```

```{r}
# LDA with Lag2 interaction with Lag1
lda.fit = lda(Direction ~ Lag2:Lag1, data = Weekly, subset = train)
lda.pred = predict(lda.fit, Weekly.910)
mean(lda.pred$class == Direction.910)

```

```{r}
# QDA with sqrt(abs(Lag2))
qda.fit = qda(Direction ~ Lag2 + sqrt(abs(Lag2)), data = Weekly, subset = train)
qda.class = predict(qda.fit, Weekly.910)$class
table(qda.class, Direction.910)
```

```{r}
mean(qda.class == Direction.910)
```

```{r}
# KNN k =10
knn.pred = knn(train.X, test.X, train.Direction, k = 10)
table(knn.pred, Direction.910)
```

```{r}
mean(knn.pred == Direction.0910)
```

```{r}
# KNN k = 100
knn.pred = knn(train.X, test.X, train.Direction, k = 100)
table(knn.pred, Direction.910)
```

```{r}
mean(knn.pred == Direction.910)
```

-   Out of these combinations, the original logistic regression and LDA have the best performance in terms of test error rates.

### Exercise 15:

#### a)

```{r}
Power = function() {
    2^3
}
print(Power())
```

#### b)

```{r}
Power2 = function(x, a) {
    x^a
}
Power2(3, 8)
```

#### c)

```{r}
Power2(10, 3)
Power2(8, 17)
Power2(131, 3)
```

#### d)

```{r}
Power3 = function(x, a) {
    result = x^a
    return(result)
}
```

#### e)

```{r}
x = 1:10
plot(x, Power3(x, 2), log = "xy", ylab = "Log of y = x^2", xlab = "Log of x", 
    main = "Log of x^2 versus Log of x")
```

#### f)

```{r}
PlotPower = function(x, a) {
    plot(x, Power3(x, a))
}
PlotPower(1:10, 3)
```
